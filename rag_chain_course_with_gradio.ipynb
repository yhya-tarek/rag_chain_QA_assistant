{
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1401544,
          "sourceType": "datasetVersion",
          "datasetId": 819052
        }
      ],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "importing packages"
      ],
      "metadata": {
        "id": "XCVTQ3MwqUWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain_core langchain_chroma langchain_text_splitters langchain langchain_google_genai sentence_transformers langsmith gradio gdown"
      ],
      "metadata": {
        "id": "42GojDdhqUWx",
        "execution": {
          "iopub.status.busy": "2024-08-28T17:18:39.975215Z",
          "iopub.execute_input": "2024-08-28T17:18:39.975600Z",
          "iopub.status.idle": "2024-08-28T17:18:57.522349Z",
          "shell.execute_reply.started": "2024-08-28T17:18:39.975561Z",
          "shell.execute_reply": "2024-08-28T17:18:57.521218Z"
        },
        "trusted": true
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from operator import itemgetter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.documents import Document\n",
        "# from kaggle_secrets import UserSecretsClient\n",
        "from google.colab import userdata\n",
        "import gdown\n",
        "from langchain.load import dumps, loads\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "qVo_hZRRqUWx",
        "execution": {
          "iopub.status.busy": "2024-08-28T17:18:57.524480Z",
          "iopub.execute_input": "2024-08-28T17:18:57.524841Z",
          "iopub.status.idle": "2024-08-28T17:18:57.744728Z",
          "shell.execute_reply.started": "2024-08-28T17:18:57.524805Z",
          "shell.execute_reply": "2024-08-28T17:18:57.743919Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0247ee96-9d58-4975-fcb6-50e7c640d7c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_KEY\")"
      ],
      "metadata": {
        "id": "0v_PtU7rqUWy",
        "execution": {
          "iopub.status.busy": "2024-08-28T15:26:53.559466Z",
          "iopub.execute_input": "2024-08-28T15:26:53.560416Z",
          "iopub.status.idle": "2024-08-28T15:26:53.773819Z",
          "shell.execute_reply.started": "2024-08-28T15:26:53.560357Z",
          "shell.execute_reply": "2024-08-28T15:26:53.772929Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run This cell**"
      ],
      "metadata": {
        "id": "uO4YMIxqZJ4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding to find similarity between texts and find relevant"
      ],
      "metadata": {
        "id": "Fh7wQxY7qUW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class embedding:\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer('all-mpnet-base-v2')\n",
        "    def embed_documents(self,docs):\n",
        "        embeddings= self.model.encode(docs)\n",
        "        return embeddings.tolist()\n",
        "    def embed_query(self,query):\n",
        "        return self.model.encode(query).tolist()"
      ],
      "metadata": {
        "id": "3EJQuHMxqUW0",
        "execution": {
          "iopub.status.busy": "2024-08-28T15:31:16.775693Z",
          "iopub.execute_input": "2024-08-28T15:31:16.776049Z",
          "iopub.status.idle": "2024-08-28T15:31:16.785760Z",
          "shell.execute_reply.started": "2024-08-28T15:31:16.776016Z",
          "shell.execute_reply": "2024-08-28T15:31:16.784913Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ragChain:\n",
        "    generate_queries_template = \"\"\"You are an AI language model assistant. Your task is to generate five arabic\n",
        "      different versions of the given user question to retrieve relevant documents from a vector\n",
        "      database. By generating multiple perspectives on the user question, your goal is to help\n",
        "      the user overcome some of the limitations of the distance-based similarity search.\n",
        "      Provide these alternative questions separated by newlines without any additional thoughts outside the answers. Original question: {question}\"\"\"\n",
        "\n",
        "    final_rag_chain_template=\"\"\"\n",
        "      you are an AI powered QA Assistant to provide accurate, contextually relevant answers to customer questions.\n",
        "      at the end of the answer you have to thank the user.\n",
        "      the answer in arabic and in details.\n",
        "      you should answer based on the context provided:\n",
        "      {context}\n",
        "\n",
        "      what to do if answer is not included in the prompt for context:\n",
        "\n",
        "      1.you should appoligize to the user and say that you dont have the answer in your informations\n",
        "\n",
        "      for answer:\n",
        "      1. the output must be in details based on the context provided\n",
        "\n",
        "      Question: {question}\n",
        "\n",
        "      answer:\n",
        "      \"\"\"\n",
        "    url = \"https://drive.google.com/drive/u/0/folders/149jD_H7oQ3JEp6D1UBaMTNIyMOs8yz06\"\n",
        "\n",
        "    documents=[]\n",
        "    # Retrieve\n",
        "    def __init__(self):\n",
        "\n",
        "\n",
        "        if not os.path.exists(f\"{os.getcwd()}/vector\"):\n",
        "            # If it fails, initialize with documents\n",
        "            gdown.download_folder(self.url)\n",
        "\n",
        "        vector_database = Chroma(persist_directory=f\"{os.getcwd()}/vector\", embedding_function=embedding())\n",
        "        retriever=vector_database.as_retriever(search_type=\"similarity\",search_kwargs={'k':3})\n",
        "\n",
        "        google_api_key = userdata.get(\"GOOGLE_AI_STUDIO2\")\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=google_api_key,temperature=0)\n",
        "        # Multi Query: Different Perspectives\n",
        "        prompt_perspectives = ChatPromptTemplate.from_template(self.generate_queries_template)\n",
        "\n",
        "\n",
        "        generate_queries = (\n",
        "            prompt_perspectives\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "            | (lambda x: x.split(\"\\n\"))\n",
        "        )\n",
        "\n",
        "        retrieval_chain = generate_queries | retriever.map() | self.get_unique_union\n",
        "\n",
        "        final_rag_chain_prompt = ChatPromptTemplate.from_template(self.final_rag_chain_template)\n",
        "\n",
        "        final_rag_chain = (\n",
        "            RunnablePassthrough.assign(context=(lambda x: self.format_docs(x[\"context\"])))\n",
        "            | final_rag_chain_prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        rag_chain_with_source = RunnableParallel(\n",
        "            {\"context\": retrieval_chain, \"question\": RunnablePassthrough()}\n",
        "        ).assign(answer=final_rag_chain)\n",
        "        self.rag_chain_with_source=rag_chain_with_source\n",
        "\n",
        "\n",
        "    def invoke(self,question):\n",
        "        return self.rag_chain_with_source.invoke(question)\n",
        "\n",
        "\n",
        "    def format_docs(self, docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Get unique union of retrieved docs\n",
        "    def get_unique_union(self, documents: list[list]):\n",
        "      \"\"\" Unique union of retrieved docs \"\"\"\n",
        "      # Flatten list of lists, and convert each Document to string\n",
        "      flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "      # Get unique documents\n",
        "      unique_docs = list(set(flattened_docs))\n",
        "      # Return\n",
        "      return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "    def gettingDocuments(self, data_directory):\n",
        "      data = []\n",
        "      for folder in os.listdir(data_directory):\n",
        "          for file in os.listdir(f'{data_directory}/{folder}'):\n",
        "\n",
        "              file = open(f\"{data_directory}/{folder}/{file}\", \"r\",encoding=\"utf8\")\n",
        "              content = file.read()\n",
        "\n",
        "              data.append({'category': folder, 'Content': content})\n",
        "\n",
        "              file.close()\n",
        "      self.documents = [Document(page_content=text[\"Content\"]) for text in data[:40000]]\n",
        ""
      ],
      "metadata": {
        "id": "dUXx5RdY0_ae"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE\n"
      ],
      "metadata": {
        "id": "gCpozgNRPZ83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from operator import itemgetter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.documents import Document\n",
        "# from kaggle_secrets import UserSecretsClient\n",
        "from google.colab import userdata\n",
        "import gdown\n",
        "from langchain.load import dumps, loads\n",
        "import gradio as gr\n",
        "\n",
        "rag_chain_with_source = ragChain()\n",
        "\n",
        "def getting_answers(question, history=[]):\n",
        "    rag_answer = rag_chain_with_source.invoke(question)\n",
        "    answer = rag_answer[\"answer\"] + \"\\n\\n\\n\\n\\nالنصوص القريبة للسوال المطروح: \\n\" + \"\\n\".join([docs.page_content for docs in rag_answer[\"context\"]])\n",
        "    history.append((question, answer))\n",
        "\n",
        "    return history,history\n",
        "\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=getting_answers,\n",
        "    inputs=[\"text\", \"state\"],  # \"text\" for user input, \"state\" to keep track of the chat history\n",
        "    outputs=[\"chatbot\", \"state\"],  # \"chatbot\" for chat UI, \"state\" to store chat history\n",
        ")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "0csp9IicqUW5",
        "execution": {
          "iopub.status.busy": "2024-08-28T17:21:34.181572Z",
          "iopub.execute_input": "2024-08-28T17:21:34.181961Z",
          "iopub.status.idle": "2024-08-28T17:21:37.336628Z",
          "shell.execute_reply.started": "2024-08-28T17:21:34.181923Z",
          "shell.execute_reply": "2024-08-28T17:21:37.335739Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "71ecdeec-24d6-4769-b76a-dc1e135bc410"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://a77d32a74304b4bd84.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a77d32a74304b4bd84.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21uwTpo6BRBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}